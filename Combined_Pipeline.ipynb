{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98518ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate all files into function defintions and main.py part\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183b7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxillary functions:\n",
    "\n",
    "def get_localisation_mask(original_mask):\n",
    "    # This function converts the mask values from {0,1,...48} to {0,1} for background vs Object\n",
    "    new_mask = (original_mask != 0)*1\n",
    "    return new_mask\n",
    "\n",
    "def get_color_ratios(original_image):\n",
    "    # This function takes image (3,height, width) and return image with R/G, G/B, B/R ratios as 3 additional channels\n",
    "    r_by_g = original_image[0]/original_image[1]\n",
    "    g_by_b = original_image[1]/original_image[2]\n",
    "    b_by_r = original_image[2]/original_image[0]\n",
    "    \n",
    "    all_channels = [original_image[0], original_image[1], original_image[2], r_by_g, g_by_b, b_by_r]\n",
    "    \n",
    "    combined_image = torch.stack(all_channels, dim = 0)\n",
    "    return combined_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9448743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CustomDataset(Dataset):\\n    def __init__(self, all_frames, evaluation_mode = False):\\n        self.frames = torch.tensor(all_frames)\\n        self.evaluation_mode = evaluation_mode\\n#         self.masks = all_masks.cuda()\\n\\n    def __len__(self):\\n        return len(self.frames)\\n\\n    def __getitem__(self, idx):\\n        global net_id\\n        i,j = self.frames[idx]\\n        mode = \\'val\\' if self.evaluation_mode else \\'train\\'\\n        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/image_{j}.png\"\\n        frame = torch.tensor(plt.imread(file_path)).permute(2, 0, 1)\\n\\n        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/mask.npy\"\\n        mask = np.load(file_path)[j]\\n        return frame, mask\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loader Definitions\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, all_frames, evaluation_mode = False):\n",
    "        self.frames = torch.tensor(all_frames)\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "#         self.masks = all_masks.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global net_id\n",
    "        i,j = self.frames[idx]\n",
    "        mode = 'val' if self.evaluation_mode else 'train'\n",
    "        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/image_{j}.png\"\n",
    "        frame = torch.tensor(plt.imread(file_path)).permute(2, 0, 1)\n",
    "\n",
    "        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/mask.npy\"\n",
    "        mask = np.load(file_path)[j]\n",
    "        return frame, mask\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64c5bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'hidden', 'train', 'unlabeled', 'val']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./../Dataset_Student/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2b367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_of_vids=1000, evaluation_mode=False):\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "        if self.evaluation_mode:\n",
    "            self.mode = 'hidden'\n",
    "            start_num = 15000\n",
    "        else:\n",
    "            self.mode = 'train'\n",
    "            start_num = 0\n",
    "        self.vid_indexes = torch.tensor([i for i in range(start_num, num_of_vids + start_num)])\n",
    "        self.num_of_vids = num_of_vids\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_of_vids\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        num_hidden_frames = 11\n",
    "        num_total_frames = 22\n",
    "        x = []\n",
    "        i = self.vid_indexes[idx]\n",
    "        \n",
    "        base_dir = './../Dataset_Student/'\n",
    "        \n",
    "        filepath = f'{base_dir}{self.mode}/video_{i}/'\n",
    "        # obtain x values.\n",
    "        for j in range(num_hidden_frames):\n",
    "            x.append(torch.tensor(plt.imread(filepath + f'image_{j}.png')).permute(2, 0, 1))\n",
    "        x = torch.stack(x, 0)\n",
    "        \n",
    "        if self.evaluation_mode:\n",
    "            return x\n",
    "        \n",
    "        file_path = f\"{base_dir}{self.mode}/video_{i}/mask.npy\"\n",
    "        y = np.load(file_path)[21]  # last frame.\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd295e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 1\n",
    "\n",
    "# Create Train DataLoader\n",
    "num_videos = 2\n",
    "train_data = CustomDataset(num_videos)\n",
    "# load the data.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create Val DataLoader\n",
    "num_val_videos = 1\n",
    "val_data = CustomDataset(num_val_videos, evaluation_mode = True)\n",
    "# load the data.\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# batch_size = 8\n",
    "# num_videos = 1000\n",
    "# # num_val_videos = 1000\n",
    "\n",
    "# train_data = CreateDatasetCustom(num_videos)\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_data = CreateDatasetCustom(num_val_videos, evaluation_mode=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6c5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 3, 160, 240]) torch.Size([1, 160, 240])\n",
      "torch.Size([1, 11, 3, 160, 240]) torch.Size([1, 160, 240])\n",
      "torch.Size([1, 11, 3, 160, 240])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "for x in val_loader:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee2a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# gpu_name = 'cuda'\n",
    "# device = torch.device(gpu_name if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gpu_name = 'mps'\n",
    "device = torch.device(gpu_name if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978177a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_model(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(combined_model, self).__init__()\n",
    "        self.frame_prediction_model = DLModelVideoPrediction((11, 3, 160, 240), 64, 512, groups=4)\n",
    "        self.frame_prediction_model = nn.DataParallel(self.frame_prediction_model)\n",
    "        self.frame_prediction_model = self.frame_prediction_model.to(device)\n",
    "\n",
    "        self.image_segmentation_model = unet_model()\n",
    "        self.image_segmentation_model = nn.DataParallel(self.image_segmentation_model)\n",
    "        self.image_segmentation_model = self.image_segmentation_model.to(device)\n",
    "        \n",
    "    def load_weights(self):\n",
    "        best_model_path = './checkpoints/frame_prediction.pth'  # load saved model to restart from previous best model\n",
    "        if os.path.isfile(best_model_path):\n",
    "            print('frame prediction model weights found')\n",
    "            self.frame_prediction_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        best_model_path = './checkpoints/image_segmentation.pth'  # load saved model to restart from previous best model\n",
    "        if os.path.isfile(best_model_path):\n",
    "            print('image segmentation model weights found')\n",
    "            self.image_segmentation_model.load_state_dict(torch.load(best_model_path))\n",
    "            \n",
    "    def save_weights(self):\n",
    "        torch.save(self.frame_prediction_model.state_dict(), './checkpoints/frame_prediction.pth')\n",
    "        torch.save(self.frame_prediction_model.state_dict(), './checkpoints/image_segmentation.pth')\n",
    "        print('model weights saved successfully')\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.frame_prediction_model(x)\n",
    "#         print(x.shape)\n",
    "        x = x[:,-1]\n",
    "#         print(x.shape)\n",
    "        x = self.image_segmentation_model(x)\n",
    "#         print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775b5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# for x,y in train_loader:\n",
    "#     x = x.to(device)\n",
    "#     out = model(x)\n",
    "#     print(out.shape, y.shape)\n",
    "#     print(criterion(out, y.to(device).long()))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20496f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2be3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c44c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Instantiate frame_prediction model and segmentation_mask model\n",
    "# frame_prediction_model = DLModelVideoPrediction((11, 3, 160, 240), 64, 512, groups=4)\n",
    "# frame_prediction_model = nn.DataParallel(frame_prediction_model)\n",
    "# frame_prediction_model = frame_prediction_model.to(device)\n",
    "\n",
    "# best_model_path = './checkpoint_frame_prediction.pth'  # load saved model to restart from previous best model\n",
    "# if os.path.isfile(best_model_path):\n",
    "#     frame_prediction_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# # Instantiate frame_prediction model and segmentation_mask model\n",
    "# # image_segmentation_model = FCN(49)\n",
    "# image_segmentation_model = unet_model()\n",
    "# image_segmentation_model = nn.DataParallel(image_segmentation_model)\n",
    "# image_segmentation_model = image_segmentation_model.to(device)\n",
    "\n",
    "# # best_model_path = 'fcn_model.pth'\n",
    "# best_model_path = './image_segmentation.pth'  # load saved model to restart from previous best model\n",
    "# if os.path.isfile(best_model_path):\n",
    "#     image_segmentation_model.load_state_dict(torch.load(best_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4a50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "num_epochs = 1\n",
    "lr = 0.0001\n",
    "model = combined_model(device)\n",
    "model.load_weights()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_loader),\n",
    "                                                epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb539e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# FLOW:\n",
    "# get 11 frames of video from dataloader (optional: Data Augmentation)\n",
    "# pass it through model to get prediction for 22nd frame\n",
    "# pass prediction through segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b1e7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 3.6833: 100%|█████████████████████████| 2/2 [00:19<00:00,  9.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss 3.7138378620147705\n",
      "model weights saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.54s/it]\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "preds_per_epoch = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_loader)\n",
    "\n",
    "    for batch_x, batch_y in train_pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device).long()\n",
    "        pred_y = model(batch_x)#.long()\n",
    "        loss = criterion(pred_y, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "        train_pbar.set_description('train loss: {:.4f}'.format(loss.item()))\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss = np.average(train_loss)\n",
    "    print(f\"Average train loss {train_loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "#     torch.save(model.state_dict(), './checkpoint_frame_prediction.pth')\n",
    "    model.save_weights()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(val_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 2 == 0:\n",
    "            for batch_x in val_pbar:\n",
    "                batch_x = batch_x.to(device)\n",
    "                pred_y = model(batch_x).float()\n",
    "                preds_per_epoch.append(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a52fe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 160, 240])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_predictions = preds_per_epoch[-1]\n",
    "torch.save(latest_predictions, 'The_Big_Epochalypse_submission.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08449bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e1a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db3bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
