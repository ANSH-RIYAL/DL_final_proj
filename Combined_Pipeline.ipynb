{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate all files into function defintions and main.py part\n",
    "# from segmentation_mask import *\n",
    "# from frame_prediction import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxillary functions:\n",
    "\n",
    "def get_localisation_mask(original_mask):\n",
    "    # This function converts the mask values from {0,1,...48} to {0,1} for background vs Object\n",
    "    new_mask = (original_mask != 0)*1\n",
    "    return new_mask\n",
    "\n",
    "def get_color_ratios(original_image):\n",
    "    # This function takes image (3,height, width) and return image with R/G, G/B, B/R ratios as 3 additional channels\n",
    "    r_by_g = original_image[0]/original_image[1]\n",
    "    g_by_b = original_image[1]/original_image[2]\n",
    "    b_by_r = original_image[2]/original_image[0]\n",
    "    \n",
    "    all_channels = [original_image[0], original_image[1], original_image[2], r_by_g, g_by_b, b_by_r]\n",
    "    \n",
    "    combined_image = torch.stack(all_channels, dim = 0)\n",
    "    return combined_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader Definitions\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, all_frames, evaluation_mode = False):\n",
    "        self.frames = torch.tensor(all_frames)\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "#         self.masks = all_masks.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global net_id\n",
    "        i,j = self.frames[idx]\n",
    "        mode = 'val' if self.evaluation_mode else 'train'\n",
    "        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/image_{j}.png\"\n",
    "        frame = torch.tensor(plt.imread(file_path)).permute(2, 0, 1)\n",
    "\n",
    "        file_path = f\"./../../../scratch/{net_id}/dataset_videos/dataset/{mode}/video_{i}/mask.npy\"\n",
    "        mask = np.load(file_path)[j]\n",
    "        return frame, mask\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_of_vids=1000, evaluation_mode=False):\n",
    "        start_num = 0\n",
    "        self.vid_indexes = torch.tensor([i for i in range(start_num, num_of_vids + start_num)])\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        num_hidden_frames = 11\n",
    "        num_total_frames = 22\n",
    "        x = []\n",
    "        i = self.vid_indexes[idx]\n",
    "        mode = 'train'\n",
    "        filepath = f'../dataset/{mode}/video_{i}/'\n",
    "        # obtain x values.\n",
    "        for j in range(num_hidden_frames):\n",
    "            x.append(torch.tensor(plt.imread(filepath + f'image_{j}.png')).permute(2, 0, 1))\n",
    "        x = torch.stack(x, 0)\n",
    "        file_path = f\"../dataset/train/video_{i}/mask.npy\"\n",
    "        y = np.load(file_path)[21]  # last frame.\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "num_videos = 1000\n",
    "num_frames_per_video = 22\n",
    "\n",
    "all_frames = [[[i,j] for j in range(num_frames_per_video)] for i in range(num_videos)]\n",
    "t = []\n",
    "for i in all_frames:\n",
    "    t += i\n",
    "all_frames = torch.tensor(t)\n",
    "\n",
    "batch_size = 8\n",
    "num_videos = 3\n",
    "# Create DataLoader\n",
    "# # # train_dataset = CreateDatasetCustom(5)\n",
    "train_data = CreateDatasetCustom(num_videos)\n",
    "# load the data.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader\n",
    "num_val_videos = 1\n",
    "val_data = CreateDatasetCustom(num_val_videos, evaluation_mode = True)\n",
    "# load the data.\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_size = 8\n",
    "num_videos = 1000\n",
    "# num_val_videos = 1000\n",
    "\n",
    "train_data = CreateDatasetCustom(num_videos)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = CreateDatasetCustom(num_val_videos, evaluation_mode=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate frame_prediction model and segmentation_mask model\n",
    "model = DLModelVideoPrediction((11, 3, 160, 240), 64, 512, groups=4)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop:\n",
    "best_model_path = './checkpoint_frame_prediction.pth'  # load saved model to restart from previous best model\n",
    "if os.path.isfile(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# FLOW:\n",
    "# get 11 frames of video from dataloader (optional: Data Augmentation)\n",
    "# pass it through model to get prediction for 22nd frame\n",
    "# pass prediction through segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
