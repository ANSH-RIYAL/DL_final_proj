{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.backends.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_15.png',\n",
       " 'image_14.png',\n",
       " 'image_16.png',\n",
       " 'image_17.png',\n",
       " 'image_13.png',\n",
       " 'image_12.png',\n",
       " 'image_10.png',\n",
       " 'image_11.png',\n",
       " 'image_8.png',\n",
       " 'image_9.png',\n",
       " 'image_2.png',\n",
       " 'image_3.png',\n",
       " 'image_1.png',\n",
       " 'image_0.png',\n",
       " 'image_4.png',\n",
       " 'image_5.png',\n",
       " 'image_7.png',\n",
       " 'image_6.png',\n",
       " 'image_20.png',\n",
       " 'image_21.png',\n",
       " 'image_19.png',\n",
       " 'image_18.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # os.listdir('./../Dataset_Student/unlabeled/video_10000/')\n",
    "os.listdir('./Dataset_Student_sample/unlabeled/video_10000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 240, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "# # # base_dir = './../Dataset_Student/unlabeled/video_10000/'\n",
    "base_dir = './Dataset_Student_sample/unlabeled/video_10000/'\n",
    "\n",
    "image_names = [f'image_{i}.png' for i in range(22)]\n",
    "\n",
    "for file_name in image_names:\n",
    "    img = plt.imread(base_dir + file_name)\n",
    "    frames.append(img)\n",
    "# plt.imshow(frames[3])\n",
    "frames[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDatasetCustom(Dataset):\n",
    "    def __init__(self, num_of_vids):\n",
    "        start_num = 10000\n",
    "        self.vid_indexes = torch.tensor([i for i in range(start_num, num_of_vids + start_num)])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        num_hidden_frames = 11\n",
    "        num_total_frames = 22\n",
    "        x = []\n",
    "        y = []\n",
    "        i = self.vid_indexes[idx]\n",
    "        # # # file_path = f'./../Dataset_Student/unlabeled/video_{i}/'\n",
    "        filepath = f'./Dataset_Student_sample/unlabeled/video_{i}/'\n",
    "        # obtain x values.\n",
    "        for j in range(num_hidden_frames):\n",
    "            x.append(torch.tensor(plt.imread(filepath + f'image_{j}.png')).permute(2, 0, 1))\n",
    "        x = torch.stack(x, 0)\n",
    "        for j in range(num_hidden_frames, num_total_frames):\n",
    "            y.append(torch.tensor(plt.imread(filepath + f'image_{j}.png')).permute(2, 0, 1))\n",
    "        y = torch.stack(y, 0)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        vid_len = len(self.vid_indexes)\n",
    "        return vid_len\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_frames = 3\n",
    "# Create DataLoader\n",
    "# # # train_dataset = CreateDatasetCustom(5)\n",
    "train_data = CreateDatasetCustom(num_frames)\n",
    "# load the data.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 3, 160, 240]) torch.Size([3, 11, 3, 160, 240])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 2, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        k_size = 3\n",
    "        p_len = 1\n",
    "        self.input_encoder = nn.Sequential(nn.Conv2d(input_channels, hidden_channels, kernel_size=k_size, stride=1, padding=p_len),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2))\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(hidden_channels, hidden_channels, kernel_size=k_size, stride=2, padding=p_len),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(hidden_channels, hidden_channels, kernel_size=k_size, stride=1, padding=p_len),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Conv2d(hidden_channels, hidden_channels, kernel_size=k_size, stride=2, padding=p_len),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_encoder(x)\n",
    "        x_enc = x.clone()\n",
    "        x = self.encoder(x)\n",
    "        return x, x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupConvolution(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride, padding, groups, act_norm=False):\n",
    "        super(GroupConvolution, self).__init__()\n",
    "        self.k = kernel_size\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "        self.a_normalization = act_norm\n",
    "        if input_channels % groups != 0:\n",
    "            groups = 1\n",
    "        self.convolution = nn.Conv2d(input_channels, output_channels, kernel_size=self.k, stride=self.s, \n",
    "                              padding=self.p, groups=groups)\n",
    "        self.normalization = nn.GroupNorm(groups, output_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_label = self.convolution(x)\n",
    "        if (self.a_normalization):\n",
    "            y_norm = self.normalization(y_label)\n",
    "            y_label = self.activation(y_norm)\n",
    "        return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, inception_kernel = [3, 5, 7, 11], groups=8):        \n",
    "        super().__init__()\n",
    "        self.k_size = 1\n",
    "        self.s_size = 1\n",
    "        list_layers = []\n",
    "        self.convolution = nn.Conv2d(input_dim, hidden_dim, kernel_size=self.k_size, stride=self.s_size, padding=0)\n",
    "        for k in inception_kernel:\n",
    "            list_layers.append(GroupConvolution(hidden_dim, output_dim, kernel_size=k, stride=self.s_size, padding = k//2, \n",
    "                                      groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*list_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        y_label = 0\n",
    "        for layer in self.layers:\n",
    "            y_label += layer(x)\n",
    "        return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBridge(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, N_T, inception_kernel = [3,5,7,11], groups=8):\n",
    "        super().__init__() \n",
    "        self.N_T = N_T\n",
    "        # encoder.\n",
    "        encoder_layers = [InceptionModule(input_channels, hidden_channels//2, hidden_channels, inception_kernel = inception_kernel, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            encoder_layers.append(InceptionModule(hidden_channels, hidden_channels//2, hidden_channels, inception_kernel = inception_kernel, groups=groups))\n",
    "        encoder_layers.append(InceptionModule(hidden_channels, hidden_channels//2, hidden_channels, inception_kernel = inception_kernel, groups=groups))\n",
    "        # decoder.\n",
    "        decoder_layers = [InceptionModule(hidden_channels, hidden_channels//2, hidden_channels, inception_kernel = inception_kernel, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            decoder_layers.append(InceptionModule(2*hidden_channels, hidden_channels//2, hidden_channels, inception_kernel = inception_kernel, groups=groups))\n",
    "        decoder_layers.append(InceptionModule(2*hidden_channels, hidden_channels//2, input_channels, inception_kernel = inception_kernel, groups=groups))\n",
    "        # self vars.\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        # Encoder.\n",
    "        list_pass = []\n",
    "        z_hid = x\n",
    "        for i in range(self.N_T):\n",
    "            z_hid = self.encoder[i](z_hid)\n",
    "            if (i < self.N_T - 1):\n",
    "                list_pass.append(z_hid)\n",
    "\n",
    "        # Decoder.\n",
    "        z_hid = self.decoder[0](z_hid)\n",
    "        for i in range(1, self.N_T):\n",
    "            z_hid = self.decoder[i](torch.cat([z_hid, list_pass[-i]], dim=1))\n",
    "\n",
    "        y_label = z_hid.reshape(B, T, C, H, W)\n",
    "        return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2, 1, 2, 1]\n",
    "class CNN_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.k_size = 3\n",
    "        self.p_size = 1\n",
    "        self.output_p = 1\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=self.k_size, stride=2, padding=self.p_size, output_padding = self.output_p),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                 nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=self.k_size, stride=1, padding=self.p_size),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 \n",
    "                                 nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=self.k_size, stride=2, padding=self.p_size, output_padding = self.output_p),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2))\n",
    "        self.output_decoder = nn.Sequential(nn.ConvTranspose2d(2*hidden_channels, hidden_channels, kernel_size=self.k_size, stride=1, padding=self.p_size),\n",
    "                                 nn.GroupNorm(2, hidden_channels),\n",
    "                                 nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.output = nn.Conv2d(hidden_channels, output_channels, 1)\n",
    "        \n",
    "    def forward(self, x, encoding):\n",
    "        x = self.decoder(x)\n",
    "        y_label = self.output_decoder(torch.cat([x, encoding], dim=1))\n",
    "        y_label = self.output(y_label)\n",
    "        return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModelVideoPrediction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size=16, translator_size=256, inception_kernel=[3,5,7,11], groups=8):\n",
    "        super().__init__()\n",
    "        T, C, H, W = input_dim\n",
    "        self.encoding = CNN_Encoder(C, hidden_size)\n",
    "        self.hidden = InceptionBridge(T*hidden_size, translator_size, 8, inception_kernel, groups)\n",
    "        self.decoding = CNN_Decoder(hidden_size, C)\n",
    "\n",
    "\n",
    "    def forward(self, x_orig):\n",
    "        B, T, C, H, W = x_orig.shape\n",
    "        x = x_orig.view(B*T, C, H, W)\n",
    "\n",
    "        embed, pass_ = self.encoding(x)\n",
    "        _, C_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, H_, W_)\n",
    "        hidden_el = self.hidden(z)\n",
    "        hidden_el = hidden_el.reshape(B*T, C_, H_, W_)\n",
    "\n",
    "        Y = self.decoding(hidden_el, pass_)\n",
    "        Y = Y.reshape(B, T, C, H, W)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11, 3, 160, 240])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # model = SimVP((11, 3, 160, 240),64, 512, groups = 4)\n",
    "model = DLModelVideoPrediction((11, 3, 160, 240), 64, 512, groups = 4)\n",
    "model = model.to(device)\n",
    "result = model(x.to(device))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11, 3, 160, 240])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11, 3, 160, 240])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training Loop:\n",
    "\n",
    "best_model_path = './checkpoint.pth' # load saved model to restart from previous best model (lowest val loss) checkpoint\n",
    "\n",
    "# recorder = Recorder(verbose=True)\n",
    "if os.path.isfile(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    \n",
    "num_epochs = 2\n",
    "lr = 0.001\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_loader)\n",
    "    count = 0\n",
    "\n",
    "    for batch_x, batch_y in train_pbar:\n",
    "        #print(count)\n",
    "        optimizer.zero_grad()\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        pred_y = model(batch_x)\n",
    "\n",
    "        #pred_y_norm = pred_y / 255.0\n",
    "        #batch_y_norm = batch_y / 255.0\n",
    "\n",
    "        loss = criterion(pred_y, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "        train_pbar.set_description('train loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        count = count + 1\n",
    "        torch.cuda.empty_cache()\n",
    "        #if count == 50: \n",
    "        #    break\n",
    "    train_loss = np.average(train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "### TODO: VALIDATION\n",
    "\n",
    "#     if epoch % 10 == 0:\n",
    "#         with torch.no_grad():\n",
    "#             vali_loss = self.vali(self.vali_loader)\n",
    "#             torch.cuda.empty_cache()\n",
    "#             #if epoch % (args.log_step * 100) == 0:\n",
    "#             self._save(name=str(epoch))\n",
    "#         print_log(\"Epoch: {0} | Train Loss: {1:.4f} Vali Loss: {2:.4f}\\n\".format(\n",
    "#             epoch + 1, train_loss, vali_loss))\n",
    "#         recorder(vali_loss, self.model, self.path)\n",
    "\n",
    "#         wandb.log({'train_loss': train_loss, 'vali_loss': vali_loss, 'epoch': epoch})\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), './checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
